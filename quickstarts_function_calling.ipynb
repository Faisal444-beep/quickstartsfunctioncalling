{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NT6m9Z0YkRWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function calling lets developers create a description of a function in their code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with. Function calling lets you use functions as tools in generative AI applications, and you can define more than one function within a single request.\n",
        "\n",
        "#This notebook provides code examples to help you get started.\n",
        "\n",
        "######Install dependencies"
      ],
      "metadata": {
        "id": "MPVDfcUKkTRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\"  # Install the Python SDK"
      ],
      "metadata": {
        "id": "CpwK_UHVd8wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up your API key\n",
        "#To run the following cell, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the Authentication quickstart for an example.\n"
      ],
      "metadata": {
        "id": "IGOiLWnekZf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai # Import the google.generativeai module and alias it as genai\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY) # Now you can use genai to configure the API key\n",
        "#genai.configure(api_key=GOOGLE_API_KEY)  # This line is redundant, you only need to configure once"
      ],
      "metadata": {
        "id": "XqECHT-peNlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Function calling basics\n",
        "#To use function calling, pass a list of functions to the tools parameter when creating a GenerativeModel. The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
        "\n",
        "#Important: The SDK converts function parameter type annotations to a format the API understands (genai.protos.FunctionDeclaration). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict\n"
      ],
      "metadata": {
        "id": "HhMdNnTBkirC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a: float, b: float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: float, b: float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: float, b: float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\", tools=[add, subtract, multiply, divide]\n",
        ")\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb6ReYfDeTGg",
        "outputId": "ad32e1a5-438b-4d78-c39f-5d1092329785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-flash',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=<google.generativeai.types.content_types.FunctionLibrary object at 0x7af81c780bd0>,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatic function calling\n",
        "#Function calls naturally fit in to multi-turn chats as they capture a back and forth interaction between the user and model. The Python SDK's ChatSession is a great interface for chats because handles the conversation history for you, and using the parameter enable_automatic_function_calling simplifies function calling even further:\n"
      ],
      "metadata": {
        "id": "0Jx9f5z7kr6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ],
      "metadata": {
        "id": "Z4GYYXBQe38b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With automatic function calling enabled, ChatSession.send_message automatically calls your function if the model asks it to.\n",
        "\n",
        "#In the following example, the result appears to simply be a text response containing the correct answer:\n",
        "response = chat.send_message(\n",
        "    \"I have 57 cats, each owns 44 mittens, how many mittens is that in total?\"\n",
        ")\n",
        "response.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oSJPK8BcfVhZ",
        "outputId": "f75360f5-db47-48c9-ba6b-f2c0d3b92399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'57 cats * 44 mittens/cat = 2508 mittens in total.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "57 * 44"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbSrPWAAflzx",
        "outputId": "0b427c96-bb74-4bf2-b4ea-198ddf34089b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2508"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#However, by examining the chat history, you can see the flow of the conversation and how function calls are integrated within it.\n",
        "\n",
        "#The ChatSession.history property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a genai.protos.Content object, which contains the following information:\n",
        "\n",
        "#Role: Identifies whether the content originated from the \"user\" or the \"model\".\n",
        "#Parts: A list of genai.protos.Part objects that represent individual components of the message. With a text-only model, these parts can be:\n",
        "#Text: Plain text messages.\n",
        "#Function Call (genai.protos.FunctionCall): A request from the model to execute a specific function with provided arguments.\n",
        "#Function Response (genai.protos.FunctionResponse): The result returned by the user after executing the requested function.\n",
        "#In the previous example with the mittens calculation, the history shows the following sequence:\n",
        "\n",
        "#User: Asks the question about the total number of mittens.\n",
        "#Model: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
        "#User: The ChatSession automatically executes the function (due to enable_automatic_function_calling being set) and sends back a FunctionResponse with the calculated result.\n",
        "#Model: Uses the function's output to formulate the final answer and presents it as a text response\n"
      ],
      "metadata": {
        "id": "LelfBzkNkFwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8Ngx-YpfzxT",
        "outputId": "47aa994a-f5f6-4031-db48-0446ac29f013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': 'I have 57 cats, each owns 44 mittens, how many mittens is that in total?'}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'multiply', 'args': {'a': 57.0, 'b': 44.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 2508.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'text': '57 cats * 44 mittens/cat = 2508 mittens in total.\\n'}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LJHdRVkmjoAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general the state diagram is:\n",
        "\n",
        "#The model can always reply with text, or a FunctionCall. If the model sends a FunctionCall the user must reply with a FunctionResponse\n",
        "#The model can respond with multiple function calls before returning a text response, and function calls come before the text response.\n",
        "\n",
        "#Manual function calling\n",
        "#For more control, you can process genai.protos.FunctionCall requests from the model yourself. This would be the case if:\n",
        "\n",
        "#You use a ChatSession with the default enable_automatic_function_calling=False.\n",
        "#You use GenerativeModel.generate_content (and manage the chat history yourself).\n",
        "#The following example is a rough equivalent of the function calling single-turn curl sample in Python. It uses functions that return (mock) movie playtime information, possibly from a hypothetical API:"
      ],
      "metadata": {
        "id": "B9TND56xjxjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_movies(description: str, location: str = \"\"):\n",
        "    \"\"\"find movie titles currently playing in theaters based on any description, genre, title words, etc.\n",
        "\n",
        "    Args:\n",
        "        description: Any kind of description including category or genre, title words, attributes, etc.\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "    \"\"\"\n",
        "    return [\"Barbie\", \"Oppenheimer\"]\n",
        "\n",
        "\n",
        "def find_theaters(location: str, movie: str = \"\"):\n",
        "    \"\"\"Find theaters based on location and optionally movie title which are is currently playing in theaters.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "        movie: Any movie title\n",
        "    \"\"\"\n",
        "    return [\"Googleplex 16\", \"Android Theatre\"]\n",
        "\n",
        "\n",
        "def get_showtimes(location: str, movie: str, theater: str, date: str):\n",
        "    \"\"\"\n",
        "    Find the start times for movies playing in a specific theater.\n",
        "\n",
        "    Args:\n",
        "      location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "      movie: Any movie title\n",
        "      thearer: Name of the theater\n",
        "      date: Date for requested showtime\n",
        "    \"\"\"\n",
        "    return [\"10:00\", \"11:00\"]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MjmIYv8AgZbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Use a dictionary to make looking up functions by name easier later on. You can also use it to pass the array of functions to the tools parameter of GenerativeModel.\n"
      ],
      "metadata": {
        "id": "xBLFKNmsjcnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "functions = {\n",
        "    \"find_movies\": find_movies,\n",
        "    \"find_theaters\": find_theaters,\n",
        "    \"get_showtimes\": get_showtimes,\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=functions.values())"
      ],
      "metadata": {
        "id": "VhK5v-CPg0HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#After using generate_content() to ask a question, the model requests a function_call:\n"
      ],
      "metadata": {
        "id": "_gDMiTi5jRce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = model.generate_content(\n",
        "    \"Which theaters in Mountain View show the Barbie movie?\"\n",
        ")\n",
        "response.candidates[0].content.parts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "tqeOIvZRhDfA",
        "outputId": "226e2930-7136-403d-cdc8-e4145c9d5f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[function_call {\n",
              "  name: \"find_theaters\"\n",
              "  args {\n",
              "    fields {\n",
              "      key: \"movie\"\n",
              "      value {\n",
              "        string_value: \"Barbie\"\n",
              "      }\n",
              "    }\n",
              "    fields {\n",
              "      key: \"location\"\n",
              "      value {\n",
              "        string_value: \"Mountain View\"\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_function(function_call, functions):\n",
        "    function_name = function_call.name\n",
        "    function_args = function_call.args\n",
        "    return functions[function_name](**function_args)\n",
        "\n",
        "\n",
        "part = response.candidates[0].content.parts[0]\n"
      ],
      "metadata": {
        "id": "uRB6k2Cxie18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if it's a function call; in real use you'd need to also handle text\n",
        "# responses as you won't know what the model will respond with."
      ],
      "metadata": {
        "id": "z_v1DRvAk7c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if part.function_call:\n",
        "    result = call_function(part.function_call, functions)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dorVRI6ai882",
        "outputId": "ac9a9790-8140-4357-df7e-45ac1f39bce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Googleplex 16', 'Android Theatre']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Finally, pass the response plus the message history to the next generate_content() call to get a final text response from the model.\n"
      ],
      "metadata": {
        "id": "WXqbtJbfjH5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.protobuf.struct_pb2 import Struct\n",
        "\n",
        "# Put the result in a protobuf Struct\n",
        "s = Struct()\n",
        "s.update({\"result\": result})\n",
        "\n",
        "# Update this after https://github.com/google/generative-ai-python/issues/243\n",
        "function_response = genai.protos.Part(\n",
        "    function_response=genai.protos.FunctionResponse(name=\"find_theaters\", response=s)\n",
        ")\n",
        "\n",
        "# Build the message history\n",
        "messages = [\n",
        "    # fmt: off\n",
        "    {\"role\": \"user\",\n",
        "     \"parts\": [\"Which theaters in Mountain View show the Barbie movie?.\"]},\n",
        "    {\"role\": \"model\",\n",
        "     \"parts\": response.candidates[0].content.parts},\n",
        "    {\"role\": \"user\",\n",
        "     \"parts\": [function_response]},\n",
        "    # fmt: on\n",
        "]\n",
        "\n",
        "# Generate the next response\n",
        "response = model.generate_content(messages)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6D7UTIA-jMSv",
        "outputId": "6c3ff02e-9d14-4d8c-aa53-d21c71816af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Barbie movie is playing at the Googleplex 16 and the Android Theatre in Mountain View.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function calling chain\n",
        "The model is not limited to one function call, it can chain them until it finds the right answer."
      ],
      "metadata": {
        "id": "P6gAbc_IlSfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "response = chat.send_message(\n",
        "    \"Which comedy movies are shown tonight in Mountain view and at what time?\"\n",
        ")\n",
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "v5-MgWKHlUcb",
        "outputId": "21f12603-d5bc-405f-e759-836b5cea7445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': 'Which comedy movies are shown tonight in Mountain view and at what time?'}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'text': \"I need to know today's date to answer your question.  What is today's date?\\n\"}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parallel function calls\n",
        "The Gemini API can call multiple functions in a single turn. This caters for scenarios where there are multiple function calls that can take place independently to complete a task.\n",
        "\n",
        "First set the tools up. Unlike the movie example above, these functions do not require input from each other to be called so they should be good candidates for parallel calling."
      ],
      "metadata": {
        "id": "KO28cGX5lkpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def power_disco_ball(power: bool) -> bool:\n",
        "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
        "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
        "    \"\"\"Play some music matching the specified parameters.\n",
        "\n",
        "    Args:\n",
        "      energetic: Whether the music is energetic or not.\n",
        "      loud: Whether the music is loud or not.\n",
        "      bpm: The beats per minute of the music.\n",
        "\n",
        "    Returns: The name of the song being played.\n",
        "    \"\"\"\n",
        "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
        "    return \"Never gonna give you up.\"\n",
        "\n",
        "\n",
        "def dim_lights(brightness: float) -> bool:\n",
        "    \"\"\"Dim the lights.\n",
        "\n",
        "    Args:\n",
        "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
        "    \"\"\"\n",
        "    print(f\"Lights are now set to {brightness:.0%}\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "_YLYHJHRll_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now call the model with an instruction that could use all of the specified tools."
      ],
      "metadata": {
        "id": "hILX5B23ltU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model up with tools.\n",
        "house_fns = [power_disco_ball, start_music, dim_lights]\n",
        "# Try this out with Pro and Flash...\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
        "\n",
        "# Call the API.\n",
        "chat = model.start_chat()\n",
        "response = chat.send_message(\"Turn this place into a party!\")\n",
        "\n",
        "# Print out each of the function calls requested from this single call.\n",
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "eTvxUbeklxBL",
        "outputId": "1cc1feb1-a5c7-4a48-f36f-a99d923bd0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "power_disco_ball(power=True)\n",
            "start_music(loud=True, energetic=True, bpm=120.0)\n",
            "dim_lights(brightness=0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested."
      ],
      "metadata": {
        "id": "pbpax91zl_MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the responses from the specified tools.\n",
        "responses = {\n",
        "    \"power_disco_ball\": True,\n",
        "    \"start_music\": \"Never gonna give you up.\",\n",
        "    \"dim_lights\": True,\n",
        "}\n",
        "\n",
        "# Build the response parts.\n",
        "response_parts = [\n",
        "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
        "    for fn, val in responses.items()\n",
        "]\n",
        "\n",
        "response = chat.send_message(response_parts)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "aEvEL3MbmBCl",
        "outputId": "c175fa34-3f8b-4ec4-a50d-297266229dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ok, the disco ball is on,  \"Never gonna give you up\" is playing loudly, and the lights are dimmed to 50% brightness.  Party time!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Steps\n",
        "Useful API references:\n",
        "The genai.GenerativeModel class\n",
        "Its GenerativeModel.generate_content method builds a genai.protos.GenerateContentRequest behind the scenes.\n",
        "The request's .tools field contains a list of 1 genai.protos.Tool object.\n",
        "The tool's function_declarations attribute contains a list of FunctionDeclarations objects.\n",
        "The response may contain a genai.protos.FunctionCall, in response.candidates[0].contents.parts[0].\n",
        "if enable_automatic_function_calling is set the genai.ChatSession executes the call, and sends back the genai.protos.FunctionResponse.\n",
        "In response to a FunctionCall the model always expects a FunctionResponse.\n",
        "If you reply manually using chat.send_message or model.generate_content remember thart the API is stateless you have to send the whole conversation history (a list of content objects), not just the last one containing the FunctionResponse.\n",
        "Related examples\n",
        "Check those examples using function calling to give you more ideas on how to use that very useful feature:\n",
        "\n",
        "Barista Bot, an agent to order coffee\n",
        "Using function calling to re-rank seach results\n",
        "Continue your discovery of the Gemini API\n",
        "Learn how to control how the Gemini API interact with your functions in the function calling config quickstart, discover how to control the model output in JSON or using an Enum or learn how the Gemini API can generate and run code by itself using Code execution"
      ],
      "metadata": {
        "id": "GZrrJX0JmRMN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnTxvy-mmTeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}